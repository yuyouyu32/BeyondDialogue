# Auto Role-Playing Evaluation 

The `AutoRPEval` codebase is designed to **automatically evaluate role-playing models** across multiple dimensions by generating dialogues and assessing their quality using a range of metrics. The evaluation process involves generating new chat roles and scenarios, conducting dialogues, and applying eight distinct evaluation metrics to the results.
All eight evaluation tasks are designed as **objective assessments**, consisting of multiple-choice and true/false questions.



- **Automatic Role Generation:**
  - Use GPT-4o to generate new roles to chat with the role under evaluation.
  - Generate specific scenario settings for the dialogue (e.g., scene, relationship, intimacy level).
- **Automatic Dialogue Generation:**
  - Conduct automatic dialogues between the evaluated role and the generated role in the given scenario, with a total of 5 rounds per dialogue, forming the evaluated dialogue.
- **Automatic Evaluation:**
  - Evaluate the generated dialogue across 8 dimensions, including the 5 CSERP dimensions, aligned with the alignment task's prompts.
  - Calculate statistical and significance metrics based on the evaluation results.
- **Sampling for Alignment Task:**
  - Sample GPT-4o evaluations of the new dialogues on each CSERP dimension for alignment task evaluation and metrics calculation.
- **Ablation Studies:**
  - Perform ablation studies on dialogues and evaluations.

For detailed instructions on running this code, please refer to the `README` within the `AutoRPEval` codebase.

### 1. Chat Role and Scenario Generation
Generate different chat roles and corresponding scenarios, including emotion and relationship settings, for each test role. This step is crucial for creating diverse and realistic contexts in which the models will be evaluated.
```bash
python -m DataGen.gen_role
python -m DataGen.gen_scene
python -m DataGen.gen_emotion
python -m DataGen.gen_relationship
```

### 2. Automatic Dialogue Generation
Once the chat role and scenario are generated, the model under test assumes the role of the test character, while GPT-4o plays the chat role. The two characters engage in a dialogue within the predefined scenario, producing dialogues for evaluation.
```bash
python -m AutoDialog.auto_dialogues
```

### 3. Evaluation Metrics
- **Purpose:** Evaluate the generated dialogues across eight metrics. These metrics are designed to assess the model's ability to follow the role's profile and scenario setting, in alignment with the CSERP dimensions. Additionally, three extra metrics—Human-likeness, Coherence, and Role Choice—are introduced to further evaluate the dialogues.

#### Metrics Overview:
- **Emotion and Relationship:**
  - Rated on a scale of 0-10 by GPT-4o based on the evaluated dialogue. Scores are used to calculate the Normalized Mean Absolute Percentage Error (NMAPE) against the context labels generated by the prompt model.
- **Character, Style, and Personality:**
  - Rated by GPT-4o based on the dialogue, referencing labels from the role profile. Personality uses the MBTI classification system and is a binary recall task, while Character and Style are multi-label recall tasks.
- **Human-likeness:**
  - Assesses whether the role-playing model provides a natural and realistic interaction experience. Converted into a true/false question, GPT-4o determines whether the dialogue is human-generated or model-generated.
- **Role Choice:**
  - Evaluates the recognizability of the role presented in the dialogue. GPT-4o selects the most suitable role for the current dialogue from four candidate roles, with names masked.
- **Coherence:**
  - Assesses the logical consistency and contextual coherence of multi-turn dialogues. GPT-4o evaluates whether the dialogue is coherent based on the given context and scenario.

### 4. Evaluation Execution

Execute the evaluation across the eight dimensions using the generated dialogues. Detailed results can be found in the `chat_dialogues` and `chat_dialogues_en` folders. Due to file size limitations, only two examples are provided to help users understand the relevant code.
```bash
./AutoTest/run.sh
```

### 5. Metrics Calculation
Based on GPT-4o's evaluation results, calculate all the evaluation metrics. The final results are stored in the `main_result` folder.
```bash
python -m Metrics.main
```

> Note: All dialogue models, evaluation models, and metric calculation models can be configured in the `./src/config.py` file.
